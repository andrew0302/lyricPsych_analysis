---
title: "questions"
author: "Andrew M. Demetriou"
date: "4/6/2020"
output:
  html_document:
    df_print: paged
  html_notebook:
    code_folding: hide
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(broom)
library(ggplot2)
library(lme4)
library(lmerTest)
library(boot)
library(gplots)
library(performance)
library(see)
library(here)

```



```{r dataset}
df <- read.csv(here("data_", "lyrics_runs.csv"))
test_df <- df %>% filter(personality=="True"|topic=="True"|linguistic=="True"|liwc=="True"|value=="True"|audio=="True")

#convert "True" and "False" to 1 and 0 respectively, for analysis
TrueFalseToNumbers <- function(x, print=TRUE){
  x <- as.character(x)
  x <- replace(x, x=="True", "1")
  x <- replace(x, x=="False", "0")
  x <- as.numeric(x)
  return(x)
}

test_df$personality <- TrueFalseToNumbers(test_df$personality)
test_df$liwc <- TrueFalseToNumbers(test_df$liwc)
test_df$topic <- TrueFalseToNumbers(test_df$topic)
test_df$value <- TrueFalseToNumbers(test_df$value)
test_df$linguistic <- TrueFalseToNumbers(test_df$linguistic)
test_df$audio <- TrueFalseToNumbers(test_df$audio)

test_df$feature_number <- (test_df$personality+test_df$topic+test_df$linguistic+test_df$liwc+test_df$value+test_df$audio)

test_df$dimension_number <- (test_df$personality*5+test_df$topic*25+test_df$linguistic*9+test_df$liwc*72+test_df$value*49+test_df$audio*240)

lapply(test_df[,c('personality', 'topic', 'value', 'audio', 'linguistic', 'liwc')], as.factor)
```

```{r}
#standardize score within task
test_df <- 
  test_df %>%
  group_by(task) %>%
  mutate(score_z = scale(score))

#log
test_df <-
  test_df %>%
  mutate(score_log = logit(score))
```


This dataset is the result of a series of runs of various system setups on three MIR tasks. The response variable, "score", is the output of each run where a higher score was more successful. The "task" column is a nominal variable representing each of the three tasks: genre classification, autotagging, or recommender system. Three systems were used for each task, and are in the "models" column as a nominal variable. 

The columns "personality", "liwc", "topic", "value", "linguistic" and "audio" represent the feature sets used in The values in these columns are either "True" or "False" depending on whether or not they were used in a given trial run. There were 5 trials run for each configuration. 


```{r head}
head(df)
```

## question 1: should we adjust the response variable? and if so, what approach should we take?

Our goal is to evaluate whether including the feature sets is useful, based on each MIR task. As such, we plan to use heirarchical linear models, with random effects specified for the task, the model, and for the interaction of model and task. 

The distributions look skewed, and the skew seems to vary both by task and model. We tried standardizing within task, and we tried applying the logit function to the raw score. 

```{r}
ggplot(test_df, aes(x = score, color = model, fill=model)) +
  geom_histogram(bins = 1000) +
  facet_wrap(~model)

ggplot(test_df, aes(x = score_z, color = model, fill=model)) +
  geom_histogram(bins = 1000) +
  facet_wrap(~model)

ggplot(test_df, aes(x = score_log, color = model, fill=model)) +
  geom_histogram(bins = 1000) +
  facet_wrap(~model)
```

```{r, echo=FALSE}
ggplot(test_df, aes(x = score, color = task, fill=task)) +
  geom_histogram(bins = 50) +
  facet_wrap(~task)

ggplot(test_df, aes(x = score_z, color = task, fill=task)) +
  geom_histogram(bins = 50) +
  facet_wrap(~task)

ggplot(test_df, aes(x = score_log, color = task, fill=task)) +
  geom_histogram(bins = 50) +
  facet_wrap(~task)
```

```{r}
alpha <- .05
ci_score <- 
  test_df %>% 
  group_by(task, model) %>%
  dplyr::summarize(mean = mean(score),
                   uci_score = mean(score) - qt(1-alpha/2, (n()-1))*sd(score)/sqrt(n()),
                   lci_score = mean(score) + qt(1-alpha/2, (n()-1))*sd(score)/sqrt(n()))

ci_score %>%
  ggplot(aes(x = task, y =mean, fill = model)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(aes(ymin = lci_score, ymax = uci_score), position = "dodge")

alpha <- .05
ci_score_z <- 
  test_df %>% 
  group_by(task, model) %>%
  dplyr::summarize(mean = mean(score_z),
                   uci_score_z = mean(score_z) - qt(1-alpha/2, (n()-1))*sd(score_z)/sqrt(n()),
                   lci_score_z = mean(score_z) + qt(1-alpha/2, (n()-1))*sd(score_z)/sqrt(n()))

ci_score_z %>%
  ggplot(aes(x = task, y =mean, fill = model)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(aes(ymin = lci_score_z, ymax = uci_score_z), position = "dodge")

alpha <- .05
ci_score_log <- 
  test_df %>% 
  group_by(task, model) %>%
  dplyr::summarize(mean = mean(score_log),
                   uci_score_log = mean(score_log) - qt(1-alpha/2, (n()-1))*sd(score_log)/sqrt(n()),
                   lci_score_log = mean(score_log) + qt(1-alpha/2, (n()-1))*sd(score_log)/sqrt(n()))

ci_score_log %>%
  ggplot(aes(x = task, y =mean, fill = model)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(aes(ymin = lci_score_log, ymax = uci_score_log), position = "dodge")

```

## question 2: how do we decide how to specify the models?

We started by running models on the scores that were standardized within task. Firstly, do we still specify task as a fixed effect? We do want to know the effect of the interaction between task and various features on the score. 

We also came across two ways of dealing with the flexibility of specifying heirarchical models:

Barr et al. argue that multi-level models should be maximally specified:
10.1016/j.jml.2012.11.001.Random 

Bates et al. argue that the random effects should be iteratively reduced by comparing AIC/BIC indices:
http://arxiv.org/abs/1506.04967. 

We employ the Bates et al. procedure by specifying a maximal model, and remove variance components in groups based on which ones show the smallest contribution. When model fit begins to decrease after the removal of terms, we settle on the most parsimonious, yet best fitting model. We then use the step() function from the lmerTest package to automatically run a similar procedure on the fixed effects. 

We also had a model that we had specified before running the experiment. 

```{r}
#maximal model
load(here("models", "m0.1.rda"))

#model after random and fixed parameter elimination procedures
load(here("models", "f1.0.rda"))

#our hypothesized model
load(here("models", "h2.2.rda"))

anova(m0.1, f1.0)
```

## question 3: how do we interpret models that have diagnostic issues?

Looking at the Q-Q plot we see sharp curves at the beginning and end of the line. We also see some issues with homoscdasticity and homogeneity of variance. 

```{r}
check_model(f1.0)
```

## question 4: how do we deal with models not converging - e.g. negative eigenvalues?

We're now considering a multiverse analysis, using the same parameter elimination procedures, starting with the raw scores. However, while implementing the procedure to reduce random effects parameters, we see model errors. Should we continue with the procedure? Or perform some other operation to get the model to converge before examining variance components for elimination?