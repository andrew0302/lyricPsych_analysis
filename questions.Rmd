---
title: "questions"
author: "Andrew M. Demetriou"
date: "4/6/2020"
output:
  html_document:
    df_print: paged
  html_notebook:
    code_folding: hide
---

```{r setup, results=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(broom)
library(ggplot2)
library(lme4)
library(lmerTest)
library(boot)
library(gplots)
library(performance)
library(see)
library(here)

```



```{r dataset, results=FALSE}
df <- read.csv(here("data_", "lyrics_runs.csv"))
test_df <- df %>% filter(personality=="True"|topic=="True"|linguistic=="True"|liwc=="True"|value=="True"|audio=="True")

#convert "True" and "False" to 1 and 0 respectively, for analysis
TrueFalseToNumbers <- function(x, print=TRUE){
  x <- as.character(x)
  x <- replace(x, x=="True", "1")
  x <- replace(x, x=="False", "0")
  x <- as.numeric(x)
  return(x)
}

test_df$personality <- TrueFalseToNumbers(test_df$personality)
test_df$liwc <- TrueFalseToNumbers(test_df$liwc)
test_df$topic <- TrueFalseToNumbers(test_df$topic)
test_df$value <- TrueFalseToNumbers(test_df$value)
test_df$linguistic <- TrueFalseToNumbers(test_df$linguistic)
test_df$audio <- TrueFalseToNumbers(test_df$audio)

test_df$feature_number <- (test_df$personality+test_df$topic+test_df$linguistic+test_df$liwc+test_df$value+test_df$audio)

test_df$dimension_number <- (test_df$personality*5+test_df$topic*25+test_df$linguistic*9+test_df$liwc*72+test_df$value*49+test_df$audio*240)

lapply(test_df[,c('personality', 'topic', 'value', 'audio', 'linguistic', 'liwc')], as.factor)
```

```{r, results=FALSE}
#standardize score within task
test_df <- 
  test_df %>%
  group_by(task) %>%
  mutate(score_z = scale(score))

#log
test_df <-
  test_df %>%
  mutate(score_log = logit(score))
```


This dataset is the result of a series of runs of various system setups on three MIR tasks. The response variable, "score", is the output of each run where a higher score was more successful. The "task" column is a nominal variable representing each of the three tasks: genre classification, autotagging, or recommender system. Three systems were used for each task, and are in the "models" column as a nominal variable. 

The columns "personality", "liwc", "topic", "value", "linguistic" and "audio" represent the feature sets used in The values in these columns are either "True" or "False" depending on whether or not they were used in a given trial run. There were 5 trials run for each configuration. 


```{r head}
head(df)
```
![diagram of data structure](image.png)

## question 1: should we adjust the response variable? and if so, what approach should we take?

Our goal is to evaluate whether including the feature sets is useful, based on each MIR task. As such, we plan to use heirarchical linear models, with random effects specified for the task, the model, and for the interaction of model and task. 

The distributions look skewed, and the skew seems to vary both by task and model. We tried standardizing within task, and we tried applying the logit function to the raw score. 

Histograms showing raw score, standardized score, and logit score within each system:
```{r}
ggplot(test_df, aes(x = score, color = model, fill=model)) +
  geom_histogram(bins = 1000) +
  facet_wrap(~model)

ggplot(test_df, aes(x = score_z, color = model, fill=model)) +
  geom_histogram(bins = 1000) +
  facet_wrap(~model)

ggplot(test_df, aes(x = score_log, color = model, fill=model)) +
  geom_histogram(bins = 1000) +
  facet_wrap(~model)
```

Histograms showing raw score, standardized score, and logit score within each task:
```{r, echo=FALSE}
ggplot(test_df, aes(x = score, color = task, fill=task)) +
  geom_histogram(bins = 50) +
  facet_wrap(~task)

ggplot(test_df, aes(x = score_z, color = task, fill=task)) +
  geom_histogram(bins = 50) +
  facet_wrap(~task)

ggplot(test_df, aes(x = score_log, color = task, fill=task)) +
  geom_histogram(bins = 50) +
  facet_wrap(~task)
```
95% CI for each model, within each task, for the raw score, standardized score, and logit score respectively:
```{r}
alpha <- .05
ci_score <- 
  test_df %>% 
  group_by(task, model) %>%
  dplyr::summarize(mean = mean(score),
                   uci_score = mean(score) - qt(1-alpha/2, (n()-1))*sd(score)/sqrt(n()),
                   lci_score = mean(score) + qt(1-alpha/2, (n()-1))*sd(score)/sqrt(n()))

ci_score %>%
  ggplot(aes(x = task, y =mean, fill = model)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(aes(ymin = lci_score, ymax = uci_score), position = "dodge")

alpha <- .05
ci_score_z <- 
  test_df %>% 
  group_by(task, model) %>%
  dplyr::summarize(mean = mean(score_z),
                   uci_score_z = mean(score_z) - qt(1-alpha/2, (n()-1))*sd(score_z)/sqrt(n()),
                   lci_score_z = mean(score_z) + qt(1-alpha/2, (n()-1))*sd(score_z)/sqrt(n()))

ci_score_z %>%
  ggplot(aes(x = task, y =mean, fill = model)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(aes(ymin = lci_score_z, ymax = uci_score_z), position = "dodge")

alpha <- .05
ci_score_log <- 
  test_df %>% 
  group_by(task, model) %>%
  dplyr::summarize(mean = mean(score_log),
                   uci_score_log = mean(score_log) - qt(1-alpha/2, (n()-1))*sd(score_log)/sqrt(n()),
                   lci_score_log = mean(score_log) + qt(1-alpha/2, (n()-1))*sd(score_log)/sqrt(n()))

ci_score_log %>%
  ggplot(aes(x = task, y =mean, fill = model)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(aes(ymin = lci_score_log, ymax = uci_score_log), position = "dodge")

```

However, our attempts to run the models with either the raw score or the logit of the raw score resulted in models not converging (i.e. Hessian issues, or negative eigenvalues). We consulted this site for strategies to converge the models (beyond simply increasing the number of iterations): https://rdrr.io/cran/lme4/man/convergence.html

All attempts resulted in similar errors. See sample code below:

```{r}
#decrease stopping tolerance
#strict_tol <- lmerControl(optCtrl=list(xtol_abs=1e-8, ftol_abs=1e-8))
#if (all(l0.2@optinfo$optimizer=="nloptwrap")) {
#  l0.2.tol <- update(l0.2, control=strict_tol)
  
#recompute Hessian and gradient
#  devfun <- update(l0.2.tol, devFunOnly=TRUE)
#if (isLMM(l0.2.tol)) {
#  pars <- getME(l0.2.tol,"theta")
#} else {
#  ## GLMM: requires both random and fixed parameters
#  pars <- getME(l0.2.tol, c("theta","fixef"))
#}
#if (require("numDeriv")) {
#  cat("hess:\n"); print(hess <- hessian(devfun, unlist(pars)))
#  cat("grad:\n"); print(grad <- grad(devfun, unlist(pars)))
#  cat("scaled gradient:\n")
#  print(scgrad <- solve(chol(hess), grad))
#}
  
#restart the fit
#  l0.2.restart <- update(l0.2.tol, start=pars)
#set.seed(101)
#pars_x <- runif(length(pars),pars/1.01,pars*1.01)
#l0.2.tol.restart2 <- update(l0.2.tol, start=pars_x,
#                            control=strict_tol)
#} 
```

The most successful strategy was centering and scaling the response variable. Are we now justified in proceeding with only this treatment of the response variable?


## question 2: how do we decide how to specify the random slopes of the models?


Firstly, we specify a random structure based on what we know about the structure of the data. We know that each task represents a grouping, and that each model represents a grouping within each task. So we specify these as random intercepts. Further, we know that some models are only present within one task, so we also specify an interaction in the random effects to indicate this nested structure:

1 + (1|task), 1 + (1|model), 1 + (1|task:model)

We further want to estimate task as a fixed effect, because we expect that some feature sets will peform better on some tasks. Therefore, we would like to compute interactions between task and fixed effects. However, it's not clear if we should also specify task and/or model as a random slope as well. 


## question 3: how do we decide which model to interpret?

We also came across two ways of dealing with the flexibility of specifying heirarchical models:

Barr et al. argue that multi-level models should be maximally specified:
10.1016/j.jml.2012.11.001.Random 

Bates et al. argue that the random effects should be maximally specified and then iteratively reduced comparing AIC/BIC indices:
http://arxiv.org/abs/1506.04967. 

We employ the Bates et al. procedure by specifying a maximal model, and remove variance components in groups based on which ones show the smallest contribution. When model fit begins to decrease after the removal of terms, we settle on the most parsimonious, yet best fitting model. We then use the step() function from the lmerTest package to automatically run a similar procedure on the fixed effects. 

We also had a model that we had specified based on the structure of the data before running the experiment. As each of the feature sets is a binary variable indicating whether or not it had been used in the trial to gather the score, we specify the fixed effects of the maximal model as each of the feature sets (personality, values, liwc, linguistic, topic), the specific task, the six way interaction between the feature sets and task. The random intercepts are specified as the task, the system used (referred to as "model" in the dataset), and the interaction between task and model. The random slopes are specified as the five feature sets + task, as per Bates et al. We use the Bates et al procedure to reduce the random effects structure. We used a "leave one out" process for the fixed effects, by using the "step" function in the lmerTest package. 


This process results in three models:

```{r}
#maximal model
load(here("models", "m0.1.rda"))

#model after random and fixed parameter elimination procedures
load(here("models", "f1.0.rda"))

#our hypothesized model, with fixed parameter elimination procedure
load(here("models", "h2.2.rda"))

anova(m0.1, f1.0, h2.2)
```

Which model to we interpret? Or do we report all three?


## question 4: how do we interpret models that have diagnostic issues?

Looking at the Q-Q plot we see sharp curves at the beginning and end of the line. We also see some issues with homoscdasticity and homogeneity of variance. How do we correctly interpret the model, given the following issues?

```{r}
check_model(f1.0)
```
