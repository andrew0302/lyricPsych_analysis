---
title: "Butter Lyrics Analysis_1.1"
author: "Andrew Demetriou, Jaehun Kim"
date: "28/7/2020"
output:
  html_document:
    df_print: paged
---

We use the renv() package to ensure reproducibility of the analyses and the plots. While this has certain benefits, the cost is the time it takes to install the environment, and disk space which we estimate at approximately 3gb at the the of this script version. 

```{r setup, results=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(broom.mixed)
library(purrr)
library(ggplot2)
library(sjPlot)
library(cowplot)
library(lme4)
library(lmerTest)
library(boot)
library(gplots)
library(see)
library(here)
library(Hmisc)
library(effects)
library(viridis)
library(parallel)
library(ggpubr)

numCores <- (detectCores()-1)
#renv::init()
#renv::snapshot()
renv::restore()
```

```{r dataset}
df <- read.csv(here("data_", "lyrics_runs.csv"))
test_df <- df %>% filter(personality=="True"|topic=="True"|linguistic=="True"|liwc=="True"|value=="True"|audio=="True")

#convert "True" and "False" to 1 and 0 respectively, for analysis
TrueFalseToNumbers <- function(x, print=TRUE){
  x <- as.character(x)
  x <- replace(x, x=="True", "1")
  x <- replace(x, x=="False", "0")
  x <- as.numeric(x)
  return(x)
}

test_df$personality <- TrueFalseToNumbers(test_df$personality)
test_df$liwc <- TrueFalseToNumbers(test_df$liwc)
test_df$topic <- TrueFalseToNumbers(test_df$topic)
test_df$value <- TrueFalseToNumbers(test_df$value)
test_df$linguistic <- TrueFalseToNumbers(test_df$linguistic)
test_df$audio <- TrueFalseToNumbers(test_df$audio)

test_df$feature_number <- (test_df$personality+test_df$topic+test_df$linguistic+test_df$liwc+test_df$value+test_df$audio)

test_df$dimension_number <- (test_df$personality*5+test_df$topic*25+test_df$linguistic*9+test_df$liwc*72+test_df$value*49+test_df$audio*240)

lapply(test_df[,c('personality', 'topic', 'value', 'audio', 'linguistic', 'liwc')], as.factor)
```

This dataset is the result of a series of runs of various system setups on three MIR tasks. The response variable, "score", is the output of each run where a higher score was more successful. The "task" column is a nominal variable representing each of the three tasks: genre classification, autotagging, or recommender system. The "models" column as a nominal variable, representing the three systems that were used for each task. 

The columns "personality", "liwc", "topic", "value", "linguistic" and "audio" represent the feature sets used in each of the trial runs. The values in these columns are either "True" or "False" depending on whether or not they were used in a given trial run. There were 5 trials run for each configuration, indicated in the trial column. 

```{r head}
head(df)
```
This diagram illustrates the setup. 

![diagram of data structure](image.png)

Because the 'score' dependent varies based on how it is computed for each task, we standardized the scores within each task. 

```{r, results=FALSE}
#standardize score within task
test_df <- 
  test_df %>%
  group_by(task) %>%
  mutate(score_z = scale(score))
```

Here we show histograms of the raw and standardized dependent variable within each system. Note that "model" below refers to the system used:

```{r}
ggplot(test_df, aes(x = score, color = model, fill=model)) +
  geom_histogram(bins = 1000) +
  facet_wrap(~model)

ggplot(test_df, aes(x = score_z, color = model, fill=model)) +
  geom_histogram(bins = 1000) +
  facet_wrap(~model)
```

Histograms showing raw score, standardized score within each task:

```{r, echo=FALSE}
ggplot(test_df, aes(x = score, color = task, fill=task)) +
  geom_histogram(bins = 50) +
  facet_wrap(~task)

ggplot(test_df, aes(x = score_z, color = task, fill=task)) +
  geom_histogram(bins = 50) +
  facet_wrap(~task)
```

Here's another illustration, using 95% confidence intervals for each model, within each task, for the raw and standardized scores. Again, note that "model" refers to the system used in each MIR task:

```{r}
alpha <- .05
ci_score <- 
  test_df %>% 
  group_by(task, model) %>%
  dplyr::summarize(mean = mean(score),
                   uci_score = mean(score) - qt(1-alpha/2, (n()-1))*sd(score)/sqrt(n()),
                   lci_score = mean(score) + qt(1-alpha/2, (n()-1))*sd(score)/sqrt(n()))

ci_score %>%
  ggplot(aes(x = task, y =mean, fill = model)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(aes(ymin = lci_score, ymax = uci_score), position = "dodge")

alpha <- .05
ci_score_z <- 
  test_df %>% 
  group_by(task, model) %>%
  dplyr::summarize(mean = mean(score_z),
                   uci_score_z = mean(score_z) - qt(1-alpha/2, (n()-1))*sd(score_z)/sqrt(n()),
                   lci_score_z = mean(score_z) + qt(1-alpha/2, (n()-1))*sd(score_z)/sqrt(n()))

ci_score_z %>%
  ggplot(aes(x = task, y =mean, fill = model)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(aes(ymin = lci_score_z, ymax = uci_score_z), position = "dodge")

```

Given the structure of the data, we evaluated the following models. We know that there is a 'structure' to our data that makes our observations non-independent: most obviously, is that certain models are used within certain tasks. The score will also vary by task, even though we standardized the score within task. So we chose to estimate two sets of 'random effects strutures' denoted by the following syntax, where "model" refers to the system used for each MIR task:

1) (model|task/model)
2) (1|task/model)

see https://cran.r-project.org/web/packages/lme4/lme4.pdf for details. 

A number of models resulted in errors related to negative eigenvalues, which make interpretation of those model questionable. These models were not interpreted any further. 

Five converged with singularity warnings. This occurs when variance components - our 'random effects' - are estimated as being 0, or very close to 0. However, since we do not interpret the random effects, we proceeded with including the models with these warnings in our analysis. 

It is worth noting that experts do not agree on how to treat singularity warnings. There are a number of proposed solutions described in various papers, that are conveniently at this link, under the heading "Singular models: random effect variances estimated as zero, or correlations estimated as +/- 1": https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#singular-models-random-effect-variances-estimated-as-zero-or-correlations-estimated-as---1

It is likely that the warnings occur because of the first reason cited: "small numbers of random-effect levels", given our 3 tasks, and 6 models. However, as it does not affect how the parameter estimates of 'fixed effects' we proceed:


```{r}
#hypothesized model #1
stan7 <- lmer(score_z ~ personality + linguistic + topic + liwc + value + task + audio + 
                topic:liwc + topic:task + liwc:task + value:task + 
                (model|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

#with only two interactions
stan7.1 <- lmer(score_z ~ personality + linguistic + topic + liwc + value + task + audio + 
                  liwc:task + value:task + 
                  (model|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

#only fixed effect reduction interaction
stan7.2 <- lmer(score_z ~ personality + linguistic + topic + liwc + value + task + audio + 
                  liwc:task + 
                  (model|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

#without interactions
stan7.3 <- lmer(score_z ~ personality + linguistic + topic +liwc + value + task + audio +
                  (model|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))
#doesn't converge

#without task or interactions
stan7.4 <- lmer(score_z ~ personality + linguistic + topic +liwc + value + audio +
                  (model|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))
#fails

#hypothesized model #2
stan8 <- lmer(score_z ~ personality + linguistic + topic + liwc + value + task + audio + 
                topic:liwc + topic:task + liwc:task + value:task + 
                (1|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

#with only two interactions
stan8.1 <- lmer(score_z ~ personality +linguistic + topic + liwc + value + task + audio + 
                  liwc:task + value:task + 
                  (1|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

#with only one interaction
stan8.2 <- lmer(score_z ~ personality +linguistic + topic + liwc + value + task + audio + 
                  liwc:task +
                  (1|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

#without interactions
stan8.3 <- lmer(score_z ~ personality +linguistic + topic + liwc + value + task + audio + 
                  (1|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

#without task or interactions
stan8.4 <- lmer(score_z ~ personality +linguistic + topic + liwc + value + audio + 
                  (1|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

```

Here we list the models included in our analysis, compute confidence intervals of all the parameter estimates of all models, and coerce into a table for plotting. As it may take some time to compute the confidence intervals, we include this abbreviated cell with the commands necessary to proceed without re-running everything:

```{r}
base::load(here("all_params_stan.rda"))
model_fits_stan =list(stan8.4, stan8.3, stan8.2, stan8.1, stan7.2)
```

If you use the abbreviated cell, skip the cell below. If you wish to re-run the proceedure, then click below. 

```{r, results=FALSE}
model_fits_stan =list(stan8.4, stan8.3, stan8.2, stan8.1, stan7.2)

model_params_stan <- model_fits_stan %>%
  tibble() %>%
  rename("model" = ".") %>%
  mutate(tidied = purrr::map(model, broom::tidy),
         model_num = row_number()) %>%
  select(model_num, tidied) %>%
  unnest()
model_params_stan$dv = "standardized"

fixed_params <- model_params_stan %>% 
  filter(effect=="fixed")
ran_params <- model_params_stan %>% 
  filter(effect=="ran_pars")
ran_params$term <- paste(ran_params$group, "_", ran_params$term)
all_params <- rbind(ran_params, fixed_params)

model_confints = map(model_fits_stan, ~confint.merMod(.x, level=.95, method="boot", nsim=500, parallel="multicore", ncpus=numCores)) %>%
  tibble() %>%
  rename("model" = ".") %>%
  mutate(tidied = purrr::map(model, broom::tidy), 
         model_num = row_number()) %>%
  select(model_num, tidied) %>%
  unnest() %>%
  rename("term" = ".rownames", 
         "lower" = "X2.5..", 
         "upper" = "X97.5..")

all_params <- left_join(all_params, model_confints)
save(all_params, file = "all_params_stan.rda")
```

Some tidying in order to plot the data:

```{r}
model_fits_all = model_fits_stan %>%
  tibble() %>%
  rename("model" = ".") %>%
  mutate(model_num = row_number(),
         AIC = map_dbl(model, AIC),
         BIC = map_dbl(model, BIC)) %>%
  select(-model)

(models.sca = all_params %>%
    select(model_num, term, estimate) %>%
    spread(term, estimate) %>%
    left_join(., model_fits_all) %>%
    arrange(AIC)%>%
    select(audio, liwc, linguistic, topic, value, personality, everything(), -AIC, -BIC, -contains("Intercept"), -contains("Residual"), -contains("cor")))

variable.names = names(select(models.sca, -model_num))
```


Here we compose the first results plot in our paper. The top plot is a collection of the bootstrapped parameter estimates of each of the models. The bottom plot shows which parameters were estimated in each model. 

```{r, results=FALSE}
top = all_params %>%
  filter(term=="value"|term=="liwc"|term=="audio"|term=="personality"|term=="topic"|term=="linguistic") %>%
  ggplot(aes(model_num, estimate, color = term)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), position="dodge") +
  geom_point(position=position_dodge(width=.9)) +
  scale_colour_manual(values = inferno(6, alpha = 1, begin = .1, end = .9, direction = 1), guide = "legend") +
  scale_fill_manual(values = inferno(6, alpha = 1, begin=.1, end=.9, direction=1), guide=FALSE) +
  labs(x = "", y = "parameter estimate\n") + 
  theme_minimal(base_size = 11) +
  scale_x_continuous(breaks = seq(1, 8, by= 1)) +
  theme(text = element_text(size=17), 
        legend.title = element_text(size = 13),
        legend.text = element_text(size = 11),
        axis.text = element_text(color = "black"),
        axis.line = element_line(colour = "black"),
        legend.position = "right",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank())
top


model_fits_all = model_fits_stan %>%
  tibble() %>%
  rename("model" = ".") %>%
  mutate(model_num = row_number(),
         AIC = map_dbl(model, AIC),
         BIC = map_dbl(model, BIC)) %>%
  select(-model)

(models.sca = all_params %>%
    select(model_num, term, estimate) %>%
    spread(term, estimate) %>%
    left_join(., model_fits_all) %>%
    arrange(AIC)%>%
    select(audio, liwc, linguistic, topic, value, personality, everything(), -AIC, -BIC, -contains("Residual"), -contains("_ sd__M"), -contains("cor")))


bottom_plot <- models.sca %>%
  select(-taskrec_item_cold) %>%
  rename(
    `task[MGC]`=taskgenre_clf,
    `liwc*task[MGC]`=`liwc:taskgenre_clf`,
    `liwc*task[MR]`=`liwc:taskrec_item_cold`,
    `value*task[MGC]`=`value:taskgenre_clf`,
    `value*task[MR]`=`value:taskrec_item_cold`,
    `task (intercept*)` = `task _ sd__(Intercept)`,
    `model*task (intercept*)` = `model:task _ sd__(Intercept)`)

variable.names = colnames(select(bottom_plot, -model_num))

slope_line <- data.frame(5, "model*task (model-slope*)", "|")
names(slope_line) <- c("model_num", "variable", "value")

var_names <- c(
  'task (intercept*)', 'model*task (intercept*)', 'model*task (model-slope*)', 
  'liwc*task[MGC]', 'liwc*task[MR]', 'value*task[MGC]', 'value*task[MR]', 
  'task[MGC]', 'audio', 'liwc', 'value', 'personality', 'linguistic', 'topic',
  '(Intercept)'
)
var_colors <- c(
  'red', 'red', 'red',
  'black', 'black', 'black', 'black',
  'black', 'black', 'black', 'black', 'black', 'black', 'black',
  'black'
)

bottom = bottom_plot %>%
  gather(variable, value, eval(variable.names)) %>% 
  mutate(value = ifelse(!is.na(value), "|", "")) %>%
  rbind(slope_line) %>%
  ggplot(aes(model_num, variable)) +
  geom_text(aes(label = value)) +
  labs(x = "\nmodel number", y = "variables\n") + 
  theme_minimal(base_size = 11) +
  scale_x_continuous(breaks = seq(1, 15, by= 1)) +
  scale_y_discrete(limits=var_names, labels=var_names) +
  theme(text = element_text(size=17),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 20),
        axis.text = element_text(color = "black"),
        axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        axis.text.y = element_text(colour = var_colors, angle=0, hjust=1)
  )
bottom
  
# cowplot::plot_grid(top, bottom, ncol = 1, align = "v", axis="l", labels = c('A', 'B'))
(g <- ggarrange(top, bottom, nrow=2, labels=c('A', 'B')))

ggsave('./figure2.pdf', plot=g, width=6.5, height=6.5)
```

Here we include the code for our second results plot for the model that estimated two interaction terms between:

1) LIWC and task
2) values and task

```{r}
term.order = c(
  "personality", "linguistic", "topic", "liwc", "value",
  "taskgenre_clf", "taskrec_item_cold", "audio",
  "liwc:taskgenre_clf", "liwc:taskrec_item_cold",
  "value:taskgenre_clf", "value:taskrec_item_cold"
)
term.label = c(
  "personality", "linguistic", "topic", "liwc", "value",
  "task[MGC]", "task[MR]", "audio",
  "liwc*task[MGC]", "liwc*task[MR]",
  "value*task[MGC]", "value*task[MR]"
)

(plot.left <- get_model_data(stan8.1, type="est") %>%
  ggplot(mapping=aes(x=term, y=estimate, color=group)) +
  geom_pointrange(aes(ymin=conf.low, ymax=conf.high), size=.3) +
  labs(y = "Parameter Estimates", x = "") +
  scale_x_discrete(breaks=term.order, labels=term.label) +
  theme_light() +
  # theme(legend.position = "none") +
  theme(axis.text.y = element_text(angle=0, hjust=1),
        legend.position = "none") +
  coord_flip())

(plot.right.upper <- get_model_data(stan8.1, type="eff", terms = c("task", "liwc [0, 1]"), ci.lvl=.95) %>%
  ggplot(mapping=aes(x=x, y=predicted, color=group)) +
  geom_pointrange(aes(ymin=conf.low, ymax=conf.high), size=.3,
                  position = position_dodge2(width=.4)) +
  labs(y = "", x = "", color = "liwc") +
  scale_x_discrete(limits=c(1, 2, 3), labels=c("MAT", "MGC", "MR")) +
  theme_light() + coord_flip())

(plot.right.lower <- get_model_data(stan8.1, type="eff", terms = c("task", "value [0, 1]"), ci.lvl=.95) %>%
  ggplot(mapping=aes(x=x, y=predicted, color=group)) +
  geom_pointrange(aes(ymin=conf.low, ymax=conf.high), size=.3,
                  position = position_dodge2(width=.4)) +
  labs(y = "Marginal Prediction", x = "", color = "value") +
  scale_x_discrete(limits=c(1, 2, 3), labels=c("MAT", "MGC", "MR")) +
  theme_light() + coord_flip())

(g <- ggarrange(
  plot.left,
  ggarrange(plot.right.upper, plot.right.lower, nrow=2, labels=c("B", "C")),
  ncol=2, labels=c("A", "")
))

ggsave('./figure3.pdf', plot=g, width=6, height=3)

```

It is worth noting that dropping random effects related to task, and only estimating those related to the system e.g. (1|model), removes the singularity warnings discussed above. Although we did not include these in the paper, we did run these models, and show our code, and results below. Note that these models run without singularity warnings. 

Estimating models:
```{r}
#with only two interactions
stan9.1 <- lmer(score_z ~ personality +linguistic + topic + liwc + value + task + audio + 
                liwc:task + value:task + 
                (1|model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))
#with only one interaction
stan9.2 <- lmer(score_z ~ personality +linguistic + topic + liwc + value + task + audio + 
                  liwc:task +
                  (1|model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))
#without interactions
stan9.3 <- lmer(score_z ~ personality +linguistic + topic + liwc + value + task + audio + 
                 (1|model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))
#without task or interactions
stan9.4 <- lmer(score_z ~ personality +linguistic + topic + liwc + value + audio + 
                  (1|model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

model_fits_alt =list(stan9.4, stan9.3, stan9.2, stan9.1)
```

Extracting parameters, computing confidence intervals, coercing to table:

```{r}
model_params_alt <- model_fits_alt %>%
  tibble() %>%
  rename("model" = ".") %>%
  mutate(tidied = purrr::map(model, broom::tidy),
         model_num = row_number()) %>%
  select(model_num, tidied) %>%
  unnest()

fixed_params <- model_params_alt %>% 
  filter(effect=="fixed")
ran_params <- model_params_alt %>% 
  filter(effect=="ran_pars")
ran_params$term <- paste(ran_params$group, "_", ran_params$term)
all_params_alt <- rbind(ran_params, fixed_params)

model_confints = map(model_fits_alt, ~confint.merMod(.x, level=.95, method="boot", nsim=500, parallel="multicore", ncpus=numCores)) %>%
  tibble() %>%
  rename("model" = ".") %>%
  mutate(tidied = purrr::map(model, broom::tidy), 
         model_num = row_number()) %>%
  select(model_num, tidied) %>%
  unnest() %>%
  rename("term" = ".rownames", 
         "lower" = "X2.5..", 
         "upper" = "X97.5..")
  
all_params_alt <- left_join(all_params_alt, model_confints)
save(all_params, file = "all_params_alt.rda")

```

In the first plots, we see that our parameter estimates and therefore our conclusions, are unchanged - although, we do not interpret the specific point estimates of each model. 

Plots:
```{r}
top = all_params_alt %>%
  filter(term=="value"|term=="liwc"|term=="audio"|term=="personality"|term=="topic"|term=="linguistic") %>%
  ggplot(aes(model_num, estimate, color = term)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), position="dodge") +
  geom_point(position=position_dodge(width=.9)) +
  scale_colour_manual(values = inferno(6, alpha = 1, begin = .1, end = .9, direction = 1), guide = "legend") +
  scale_fill_manual(values = inferno(6, alpha = 1, begin=.1, end=.9, direction=1), guide=FALSE) +
  labs(x = "", y = "parameter estimate\n") + 
  theme_minimal(base_size = 11) +
  scale_x_continuous(breaks = seq(1, 8, by= 1)) +
  theme(text = element_text(size=17), 
        legend.title = element_text(size = 13),
        legend.text = element_text(size = 11),
        axis.text = element_text(color = "black"),
        axis.line = element_line(colour = "black"),
        legend.position = "top",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank())
top
```

A difference may be noted in the second set of plots. As we are not accounting for the variance related to the task as a random effect, we see a difference when it is estimated as a fixed effect in the model below. 

This can more clearly be observed if the order of the factor levels relating to task are changed: because one level of a factor is used as a reference group against which to compare other factors, changing the reference group allows for different levels of the factor to be estimated. 

Order can be adjusted with the following commented commands:
#levels(test_df$task) <- c("rec_item_cold", "genre_clf", "auto_tag")

What we observe is what appears to be a more accurate estimate for the genre classification task, and that it appears to have a small, positive effect. The other two tasks appear to be poorly estimated, given the wide confidence intervals. 

However, we chose not to include or interpret these models for three reasons: Firstly, we had standardized score within task, secondly we were aware that task was a part of the structure of our dataset, and thirdly our main effects of interest were not affected by whether or not we adjusted the random effects specifications of our model. As such, we elected not to directly interpret the parameter estimate for task in these models. 


```{r}
stan9.plot <- plot_model(stan9.1, ci.lvl=0.95)
stan9.plot <- stan9.plot + theme(text=element_text(size=16)) +
  ggtitle("") +
  labs(y="Parameter Estimates")

#levels(test_df$task) <- c("rec_item_cold", "genre_clf", "auto_tag")
#levels(test_df$task) <- c("auto_tag", "genre_clf", "rec_item_cold")

eff_liwc_task <- plot_model(stan9.1, type = "eff", terms = c("task", "liwc [0, 1]"), ci.lvl=.95) 
eff_value_task <- plot_model(stan9.1, type = "eff", terms = c("task", "value [0, 1]"), ci.lvl=.95)


right_column_plot <- cowplot::plot_grid(eff_liwc_task, eff_value_task, ncol=1, align = "v", axis="l", labels = c('B', 'C'))
plot2 <- cowplot::plot_grid(stan9.plot, right_column_plot, ncol=2, align="h", labels=c('A'))
plot2
```

To determine which terms to include in our models, we had initially used the step function from the lmerTest package. We later learned that this was suboptimal, so we adopted a more robust procedure for variable selection, which is detailed below. Importantly, this was conducted a posteriori, as a validation of our choices. 

To replicate the procedure, first download Jags version 4.x from the source:
https://sourceforge.net/projects/mcmc-jags/

```{r}
library(rjags)
library(coda)

#dat = read.csv('./projects/hier_blasso/lyrics_runs.txt')
#dat <- read.csv(here("data_", "lyrics_runs.csv"))
#dat[dat == 'True'] = 1
#dat[dat == 'False'] = 0
#dat$task = factor(dat$task)
#dat$model = factor(dat$model)
#dat <- dat %>% group_by(task) %>% mutate(z_score=scale(score))
dat <- test_df
```

Here we specify all relevant interactions:

```{r}
mod1_string = " model {
    for (i in 1:length(y)) {
        y[i] ~ dnorm(mu[i], prec)
        mu[i] = a0 + a_task[task[i]] + a_sys[task[i], sys[i]] + b[63]*audio[i]+
          b[1]*value[i]+b[2]*topic[i]+b[3]*task[i]+
          b[4]*linguistic[i]+b[5]*personality[i]+
          b[6]*liwc[i]+b[7]*value[i]*topic[i]+b[8]*value[i]*task[i]+
          b[9]*value[i]*linguistic[i]+b[10]*value[i]*personality[i]+
          b[11]*value[i]*liwc[i]+b[12]*topic[i]*task[i]+b[13]*topic[i]*linguistic[i]+
          b[14]*topic[i]*personality[i]+b[15]*topic[i]*liwc[i]+
          b[16]*task[i]*linguistic[i]+b[17]*task[i]*personality[i]+
          b[18]*task[i]*liwc[i]+b[19]*linguistic[i]*personality[i]+
          b[20]*linguistic[i]*liwc[i]+b[21]*personality[i]*liwc[i]+
          b[22]*value[i]*topic[i]*task[i]+b[23]*value[i]*topic[i]*linguistic[i]+
          b[24]*value[i]*topic[i]*personality[i]+b[25]*value[i]*topic[i]*liwc[i]+
          b[26]*value[i]*task[i]*linguistic[i]+b[27]*value[i]*task[i]*personality[i]+
          b[28]*value[i]*task[i]*liwc[i]+b[29]*value[i]*linguistic[i]*personality[i]+
          b[30]*value[i]*linguistic[i]*liwc[i]+b[31]*value[i]*personality[i]*liwc[i]+
          b[32]*topic[i]*task[i]*linguistic[i]+b[33]*topic[i]*task[i]*personality[i]+
          b[34]*topic[i]*task[i]*liwc[i]+b[35]*topic[i]*linguistic[i]*personality[i]+
          b[36]*topic[i]*linguistic[i]*liwc[i]+b[37]*topic[i]*personality[i]*liwc[i]+
          b[38]*task[i]*linguistic[i]*personality[i]+b[39]*task[i]*linguistic[i]*liwc[i]+
          b[40]*task[i]*personality[i]*liwc[i]+b[41]*linguistic[i]*personality[i]*liwc[i]+
          b[42]*value[i]*topic[i]*task[i]*linguistic[i]+
          b[43]*value[i]*topic[i]*task[i]*personality[i]+
          b[44]*value[i]*topic[i]*task[i]*liwc[i]+
          b[45]*value[i]*topic[i]*linguistic[i]*personality[i]+
          b[46]*value[i]*topic[i]*linguistic[i]*liwc[i]+
          b[47]*value[i]*topic[i]*personality[i]*liwc[i]+
          b[48]*value[i]*task[i]*linguistic[i]*personality[i]+
          b[49]*value[i]*task[i]*linguistic[i]*liwc[i]+
          b[50]*value[i]*task[i]*personality[i]*liwc[i]+
          b[51]*value[i]*linguistic[i]*personality[i]*liwc[i]+
          b[52]*topic[i]*task[i]*linguistic[i]*personality[i]+
          b[53]*topic[i]*task[i]*linguistic[i]*liwc[i]+
          b[54]*topic[i]*task[i]*personality[i]*liwc[i]+
          b[55]*topic[i]*linguistic[i]*personality[i]*liwc[i]+
          b[56]*task[i]*linguistic[i]*personality[i]*liwc[i]+
          b[57]*value[i]*topic[i]*task[i]*linguistic[i]*personality[i]+
          b[58]*value[i]*topic[i]*task[i]*linguistic[i]*liwc[i]+
          b[59]*value[i]*topic[i]*task[i]*personality[i]*liwc[i]+
          b[60]*value[i]*topic[i]*linguistic[i]*personality[i]*liwc[i]+
          b[61]*value[i]*task[i]*linguistic[i]*personality[i]*liwc[i]+
          b[62]*topic[i]*task[i]*linguistic[i]*personality[i]*liwc[i]
    }
    

    for (j in 1:3) {
        a_task[j] ~ dnorm(c_mu[j], c_sig[j])
        for (k in 1:6) {
            a_sys[j, k] ~ dnorm(0, sig_sys)
        }
    }
    a0 ~ dnorm(0.0, 1./25.)
    
    for (l in 1:63) {
        b[l] ~ ddexp(0.0, sqrt(lam_sqr)) # has variance 1.0
    }
    
    for (j in 1:3) {
        c_mu[j] ~ dnorm(0., 1./25.)
        c_sig[j] ~ dgamma(5/2., 5.*10./2.)
    }
    sig_sys ~ dgamma(5/2., 5.*10./2.)
    
    lam_sqr ~ dgamma(1, 1)
    prec ~ dgamma(5/2.0, 5*10.0/2.0)
    sig = sqrt( 1.0 / prec )
} "
```

Although we set a seed in rjags, the seed in jags itself has not been set. As such, results will vary. 

```{r}
set.seed(116)
data_jags = list(y=as.numeric(dat$score_z),
                 audio=as.numeric(dat$audio),
                 topic=as.numeric(dat$topic),
                 personality=as.numeric(dat$personality),
                 liwc=as.numeric(dat$liwc),
                 linguistic=as.numeric(dat$linguistic),
                 value=as.numeric(dat$value),
                 sys=as.numeric(dat$model),
                 task=as.numeric(dat$task))
# table(data_jags$is_oil, data_jags$region)

params = c("a0", "a_sys", "a_task", "b", "sig", "lam_sqr", "c_mu", "c_sig", "sig_sys")

mod = jags.model(textConnection(mod1_string), data=data_jags, n.chains=3)
update(mod, 1e3) # burn-in
```

The next process will take some hours to run. To look at the results of our previous run, simply load the object here:

```{r}
#base::load(here("mod_sim.rda"))
```

Otherwise run the complete process here:

```{r}
mod_sim = coda.samples(model=mod,
                       variable.names=params,
                       n.iter=1e5)

save(mod_sim, file = "mod_sim.rda")
```

Figures and visualizations:

```{r}
mod_csim = as.mcmc(do.call(rbind, mod_sim)) # combine multiple chains

## convergence diagnostics
graphics.off()
 par("mar")
 par(mar=c(1,1,1,1))
plot(mod_sim)

gelman.diag(mod_sim)
autocorr.diag(mod_sim)
autocorr.plot(mod_sim)
effectiveSize(mod_sim)

```
