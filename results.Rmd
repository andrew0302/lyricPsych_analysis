---
title: "Butter Lyrics Analysis_1.1"
author: "Andrew Demetriou, Jaehun Kim"
date: "28/7/2020"
output:
  html_document:
    df_print: paged
---

We use the renv() package to ensure reproducibility of the analyses and the plots. While this has certain benefits, the cost is the time it takes to install the environment, and disk space which we estimate at approximately 3gb at the the of this script version. 

```{r setup, results=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(broom.mixed)
library(purrr)
library(ggplot2)
library(sjPlot)
library(cowplot)
library(lme4)
library(lmerTest)
library(boot)
library(gplots)
library(see)
library(here)
library(Hmisc)
library(effects)
library(viridis)
library(parallel)
library(ggpubr)

numCores <- (detectCores()-1)
#renv::init()
#renv::snapshot()
renv::restore()
```

Loading data and adjusting variables:

```{r dataset}
df <- read.csv(here("data_", "lyrics_runs.csv"))
test_df <- df %>% filter(personality=="True"|topic=="True"|linguistic=="True"|liwc=="True"|value=="True"|audio=="True")

#convert "True" and "False" to 1 and 0 respectively, for analysis
TrueFalseToNumbers <- function(x, print=TRUE){
  x <- as.character(x)
  x <- replace(x, x=="True", "1")
  x <- replace(x, x=="False", "0")
  x <- as.numeric(x)
  return(x)
}

test_df$personality <- TrueFalseToNumbers(test_df$personality)
test_df$liwc <- TrueFalseToNumbers(test_df$liwc)
test_df$topic <- TrueFalseToNumbers(test_df$topic)
test_df$value <- TrueFalseToNumbers(test_df$value)
test_df$linguistic <- TrueFalseToNumbers(test_df$linguistic)
test_df$audio <- TrueFalseToNumbers(test_df$audio)

test_df$feature_number <- (test_df$personality+test_df$topic+test_df$linguistic+test_df$liwc+test_df$value+test_df$audio)

test_df$dimension_number <- (test_df$personality*5+test_df$topic*25+test_df$linguistic*9+test_df$liwc*72+test_df$value*49+test_df$audio*240)

lapply(test_df[,c('personality', 'topic', 'value', 'audio', 'linguistic', 'liwc')], as.factor)
```

This dataset is the result of a series of runs of various system setups on three MIR tasks. The response variable, "score", is the output of each run where a higher score was more successful. The "task" column is a nominal variable representing each of the three tasks: genre classification, autotagging, or recommender system. The "models" column as a nominal variable, representing the three systems that were used for each task. 

The columns "personality", "liwc", "topic", "value", "linguistic" and "audio" represent the feature sets used in each of the trial runs. The values in these columns are either "True" or "False" depending on whether or not they were used in a given trial run. There were 5 trials run for each configuration, indicated in the trial column. 

```{r head}
head(df)
```
This diagram illustrates the setup. 

![diagram of data structure](image.png)

Because the 'score' dependent varies based on how it is computed for each task, we standardized the scores within each task. 

```{r, results=FALSE}
#standardize score within task
test_df <- 
  test_df %>%
  group_by(task) %>%
  mutate(score_z = scale(score))
```

Here we show histograms of the raw and standardized dependent variable within each system. Note that "model" below refers to the system used:

```{r}
ggplot(test_df, aes(x = score, color = model, fill=model)) +
  geom_histogram(bins = 1000) +
  facet_wrap(~model)

ggplot(test_df, aes(x = score_z, color = model, fill=model)) +
  geom_histogram(bins = 1000) +
  facet_wrap(~model)
```

Histograms showing raw score, standardized score within each task:

```{r, echo=FALSE}
ggplot(test_df, aes(x = score, color = task, fill=task)) +
  geom_histogram(bins = 50) +
  facet_wrap(~task)

ggplot(test_df, aes(x = score_z, color = task, fill=task)) +
  geom_histogram(bins = 50) +
  facet_wrap(~task)
```

Here's another illustration, using 95% confidence intervals for each model, within each task, for the raw and standardized scores. Again, note that "model" refers to the system used in each MIR task:

```{r}
alpha <- .05
ci_score <- 
  test_df %>% 
  group_by(task, model) %>%
  dplyr::summarize(mean = mean(score),
                   uci_score = mean(score) - qt(1-alpha/2, (n()-1))*sd(score)/sqrt(n()),
                   lci_score = mean(score) + qt(1-alpha/2, (n()-1))*sd(score)/sqrt(n()))

ci_score %>%
  ggplot(aes(x = task, y =mean, fill = model)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(aes(ymin = lci_score, ymax = uci_score), position = "dodge")

alpha <- .05
ci_score_z <- 
  test_df %>% 
  group_by(task, model) %>%
  dplyr::summarize(mean = mean(score_z),
                   uci_score_z = mean(score_z) - qt(1-alpha/2, (n()-1))*sd(score_z)/sqrt(n()),
                   lci_score_z = mean(score_z) + qt(1-alpha/2, (n()-1))*sd(score_z)/sqrt(n()))

ci_score_z %>%
  ggplot(aes(x = task, y =mean, fill = model)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(aes(ymin = lci_score_z, ymax = uci_score_z), position = "dodge")

```

Given the structure of the data, we evaluated the following models. We know that there is a 'structure' to our data that makes our observations non-independent: most obviously, is that certain models are used within certain tasks. The score will also vary by task, even though we standardized the score within task. So we chose to estimate two sets of 'random effects strutures' denoted by the following syntax, where "model" refers to the system used for each MIR task:

1) (model|task/model)
2) (1|task/model)

see https://cran.r-project.org/web/packages/lme4/lme4.pdf for details. 

A number of models resulted in errors related to negative eigenvalues, which make interpretation of those model questionable. These models were not interpreted any further. 

Five converged with singularity warnings. This occurs when variance components - our 'random effects' - are estimated as being 0, or very close to 0. However, since we do not interpret the random effects, we proceeded with including the models with these warnings in our analysis. 

It is worth noting that experts do not agree on how to treat singularity warnings. There are a number of proposed solutions described in various papers, that are conveniently at this link, under the heading "Singular models: random effect variances estimated as zero, or correlations estimated as +/- 1": https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#singular-models-random-effect-variances-estimated-as-zero-or-correlations-estimated-as---1

It is likely that the warnings occur because of the first reason cited: "small numbers of random-effect levels", given our 3 tasks, and 6 models. However, as it does not affect how the parameter estimates of 'fixed effects' we proceed:


```{r}
#hypothesized model #1
stan7 <- lmer(score_z ~ personality + linguistic + topic + liwc + value + task + audio + 
                topic:liwc + topic:task + liwc:task + value:task + 
                (model|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

#with only two interactions
stan7.1 <- lmer(score_z ~ personality + linguistic + topic + liwc + value + task + audio + 
                  liwc:task + value:task + 
                  (model|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

#only fixed effect reduction interaction
stan7.2 <- lmer(score_z ~ personality + linguistic + topic + liwc + value + task + audio + 
                  liwc:task + 
                  (model|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

#without interactions
stan7.3 <- lmer(score_z ~ personality + linguistic + topic +liwc + value + task + audio +
                  (model|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))
#doesn't converge

#without task or interactions
stan7.4 <- lmer(score_z ~ personality + linguistic + topic +liwc + value + audio +
                  (model|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))
#fails

#hypothesized model #2
stan8 <- lmer(score_z ~ personality + linguistic + topic + liwc + value + task + audio + 
                topic:liwc + topic:task + liwc:task + value:task + 
                (1|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

#with only two interactions
stan8.1 <- lmer(score_z ~ personality +linguistic + topic + liwc + value + task + audio + 
                  liwc:task + value:task + 
                  (1|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

#with only one interaction
stan8.2 <- lmer(score_z ~ personality +linguistic + topic + liwc + value + task + audio + 
                  liwc:task +
                  (1|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

#without interactions
stan8.3 <- lmer(score_z ~ personality +linguistic + topic + liwc + value + task + audio + 
                  (1|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

#without task or interactions
stan8.4 <- lmer(score_z ~ personality +linguistic + topic + liwc + value + audio + 
                  (1|task/model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

```

Here we list the models included in our analysis, compute confidence intervals of all the parameter estimates of all models, and coerce into a table for plotting. As it may take some time to compute the confidence intervals, we include this abbreviated cell with the commands necessary to proceed without re-running everything:

```{r}
base::load(here("all_params_stan.rda"))
model_fits_stan =list(stan8.4, stan8.3, stan8.2, stan8.1, stan7.2)
```

If you use the abbreviated cell, skip the cell below. If you wish to re-run the proceedure, then click below. 

```{r, results=FALSE}
model_fits_stan =list(stan8.4, stan8.3, stan8.2, stan8.1, stan7.2)

model_params_stan <- model_fits_stan %>%
  tibble() %>%
  rename("model" = ".") %>%
  mutate(tidied = purrr::map(model, broom::tidy),
         model_num = row_number()) %>%
  select(model_num, tidied) %>%
  unnest()
model_params_stan$dv = "standardized"

fixed_params <- model_params_stan %>% 
  filter(effect=="fixed")
ran_params <- model_params_stan %>% 
  filter(effect=="ran_pars")
ran_params$term <- paste(ran_params$group, "_", ran_params$term)
all_params <- rbind(ran_params, fixed_params)

model_confints = map(model_fits_stan, ~confint.merMod(.x, level=.95, method="boot", nsim=500, parallel="multicore", ncpus=numCores)) %>%
  tibble() %>%
  rename("model" = ".") %>%
  mutate(tidied = purrr::map(model, broom::tidy), 
         model_num = row_number()) %>%
  select(model_num, tidied) %>%
  unnest() %>%
  rename("term" = ".rownames", 
         "lower" = "X2.5..", 
         "upper" = "X97.5..")

all_params <- left_join(all_params, model_confints)
save(all_params, file = "all_params_stan.rda")
```

Some tidying in order to plot the data:

```{r}
model_fits_all = model_fits_stan %>%
  tibble() %>%
  rename("model" = ".") %>%
  mutate(model_num = row_number(),
         AIC = map_dbl(model, AIC),
         BIC = map_dbl(model, BIC)) %>%
  select(-model)

(models.sca = all_params %>%
    select(model_num, term, estimate) %>%
    spread(term, estimate) %>%
    left_join(., model_fits_all) %>%
    arrange(AIC)%>%
    select(audio, liwc, linguistic, topic, value, personality, everything(), -AIC, -BIC, -contains("Intercept"), -contains("Residual"), -contains("cor")))

variable.names = names(select(models.sca, -model_num))
```


Here we compose the first results plot in our paper. The top plot is a collection of the bootstrapped parameter estimates of each of the models. The bottom plot shows which parameters were estimated in each model. 

```{r, results=FALSE}
top = all_params %>%
  filter(term=="value"|term=="liwc"|term=="audio"|term=="personality"|term=="topic"|term=="linguistic") %>%
  ggplot(aes(model_num, estimate, color = term)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), position="dodge") +
  geom_point(position=position_dodge(width=.9)) +
  scale_colour_manual(values = inferno(6, alpha = 1, begin = .1, end = .9, direction = 1), guide = "legend") +
  scale_fill_manual(values = inferno(6, alpha = 1, begin=.1, end=.9, direction=1), guide=FALSE) +
  labs(x = "", y = "parameter estimate\n") + 
  theme_minimal(base_size = 11) +
  scale_x_continuous(breaks = seq(1, 8, by= 1)) +
  theme(text = element_text(size=17), 
        legend.title = element_text(size = 13),
        legend.text = element_text(size = 11),
        axis.text = element_text(color = "black"),
        axis.line = element_line(colour = "black"),
        legend.position = "right",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank())
top


model_fits_all = model_fits_stan %>%
  tibble() %>%
  rename("model" = ".") %>%
  mutate(model_num = row_number(),
         AIC = map_dbl(model, AIC),
         BIC = map_dbl(model, BIC)) %>%
  select(-model)

(models.sca = all_params %>%
    select(model_num, term, estimate) %>%
    spread(term, estimate) %>%
    left_join(., model_fits_all) %>%
    arrange(AIC)%>%
    select(audio, liwc, linguistic, topic, value, personality, everything(), -AIC, -BIC, -contains("Residual"), -contains("_ sd__M"), -contains("cor")))


bottom_plot <- models.sca %>%
  select(-taskrec_item_cold) %>%
  rename(
    `task[MGC]`=taskgenre_clf,
    `liwc*task[MGC]`=`liwc:taskgenre_clf`,
    `liwc*task[MR]`=`liwc:taskrec_item_cold`,
    `value*task[MGC]`=`value:taskgenre_clf`,
    `value*task[MR]`=`value:taskrec_item_cold`,
    `task (intercept!)` = `task _ sd__(Intercept)`,
    `model*task (intercept!)` = `model:task _ sd__(Intercept)`)

variable.names = colnames(select(bottom_plot, -model_num))

slope_line <- data.frame(5, "model*task (model-slope*)", "|")
names(slope_line) <- c("model_num", "variable", "value")

var_names <- c(
  'task (intercept!)', 'model*task (intercept!)', 'model*task (model-slope!)', 
  'liwc*task[MGC]', 'liwc*task[MR]', 'value*task[MGC]', 'value*task[MR]', 
  'task[MGC]', 'audio', 'liwc', 'value', 'personality', 'linguistic', 'topic',
  '(Intercept)'
)
var_colors <- c(
  'red', 'red', 'red',
  'black', 'black', 'black', 'black',
  'black', 'black', 'black', 'black', 'black', 'black', 'black',
  'black'
)

bottom = bottom_plot %>%
  gather(variable, value, eval(variable.names)) %>% 
  mutate(value = ifelse(!is.na(value), "|", "")) %>%
  rbind(slope_line) %>%
  ggplot(aes(model_num, variable)) +
  geom_text(aes(label = value)) +
  labs(x = "\nmodel number", y = "variables\n") + 
  theme_minimal(base_size = 11) +
  scale_x_continuous(breaks = seq(1, 15, by= 1)) +
  scale_y_discrete(limits=var_names, labels=var_names) +
  theme(text = element_text(size=17),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 20),
        axis.text = element_text(color = "black"),
        axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        axis.text.y = element_text(colour = var_colors, angle=0, hjust=1)
  )
bottom
  
# cowplot::plot_grid(top, bottom, ncol = 1, align = "v", axis="l", labels = c('A', 'B'))
(g <- ggarrange(top, bottom, nrow=2, labels=c('A', 'B')))

ggsave('./figure2.pdf', plot=g, width=6.5, height=6.5)
```

Here we include the code for our second results plot for the model that estimated two interaction terms between:

1) LIWC and task
2) values and task

```{r}
term.order = c(
  "personality", "linguistic", "topic", "liwc", "value",
  "taskgenre_clf", "taskrec_item_cold", "audio",
  "liwc:taskgenre_clf", "liwc:taskrec_item_cold",
  "value:taskgenre_clf", "value:taskrec_item_cold"
)
term.label = c(
  "personality", "linguistic", "topic", "liwc", "value",
  "task[MGC]", "task[MR]", "audio",
  "liwc*task[MGC]", "liwc*task[MR]",
  "value*task[MGC]", "value*task[MR]"
)

(plot.left <- get_model_data(stan8.1, type="est") %>%
  ggplot(mapping=aes(x=term, y=estimate, color=group)) +
  geom_pointrange(aes(ymin=conf.low, ymax=conf.high), size=.3) +
  labs(y = "Parameter Estimates", x = "") +
  scale_x_discrete(breaks=term.order, labels=term.label) +
  theme_light() +
  # theme(legend.position = "none") +
  theme(axis.text.y = element_text(angle=0, hjust=1),
        legend.position = "none") +
  coord_flip())

(plot.right.upper <- get_model_data(stan8.1, type="eff", terms = c("task", "liwc [0, 1]"), ci.lvl=.95) %>%
  ggplot(mapping=aes(x=x, y=predicted, color=group)) +
  geom_pointrange(aes(ymin=conf.low, ymax=conf.high), size=.3,
                  position = position_dodge2(width=.4)) +
  labs(y = "", x = "", color = "liwc") +
  scale_x_discrete(limits=c(1, 2, 3), labels=c("MAT", "MGC", "MR")) +
  theme_light() + coord_flip())

(plot.right.lower <- get_model_data(stan8.1, type="eff", terms = c("task", "value [0, 1]"), ci.lvl=.95) %>%
  ggplot(mapping=aes(x=x, y=predicted, color=group)) +
  geom_pointrange(aes(ymin=conf.low, ymax=conf.high), size=.3,
                  position = position_dodge2(width=.4)) +
  labs(y = "Marginal Prediction", x = "", color = "value") +
  scale_x_discrete(limits=c(1, 2, 3), labels=c("MAT", "MGC", "MR")) +
  theme_light() + coord_flip())

(g <- ggarrange(
  plot.left,
  ggarrange(plot.right.upper, plot.right.lower, nrow=2, labels=c("B", "C")),
  ncol=2, labels=c("A", "")
))

ggsave('./figure3.pdf', plot=g, width=6, height=3)

```

It is worth noting that dropping random effects related to task, and only estimating those related to the system e.g. (1|model), removes the singularity warnings discussed above. Although we did not include these in the paper, we did run these models, and show our code, and results below. Note that these models run without singularity warnings. 

Estimating models:
```{r}
#with only two interactions
stan9.1 <- lmer(score_z ~ personality +linguistic + topic + liwc + value + task + audio + 
                liwc:task + value:task + 
                (1|model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))
#with only one interaction
stan9.2 <- lmer(score_z ~ personality +linguistic + topic + liwc + value + task + audio + 
                  liwc:task +
                  (1|model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))
#without interactions
stan9.3 <- lmer(score_z ~ personality +linguistic + topic + liwc + value + task + audio + 
                 (1|model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))
#without task or interactions
stan9.4 <- lmer(score_z ~ personality +linguistic + topic + liwc + value + audio + 
                  (1|model), REML=FALSE, data=test_df, control = lmerControl(optCtrl = list(maxfun = 1000000,xtol_abs=1e-8, ftol_abs=1e-8)))

model_fits_alt =list(stan9.4, stan9.3, stan9.2, stan9.1)
```

Extracting parameters, computing confidence intervals, coercing to table:

```{r}
model_params_alt <- model_fits_alt %>%
  tibble() %>%
  rename("model" = ".") %>%
  mutate(tidied = purrr::map(model, broom::tidy),
         model_num = row_number()) %>%
  select(model_num, tidied) %>%
  unnest()

fixed_params <- model_params_alt %>% 
  filter(effect=="fixed")
ran_params <- model_params_alt %>% 
  filter(effect=="ran_pars")
ran_params$term <- paste(ran_params$group, "_", ran_params$term)
all_params_alt <- rbind(ran_params, fixed_params)

model_confints = map(model_fits_alt, ~confint.merMod(.x, level=.95, method="boot", nsim=500, parallel="multicore", ncpus=numCores)) %>%
  tibble() %>%
  rename("model" = ".") %>%
  mutate(tidied = purrr::map(model, broom::tidy), 
         model_num = row_number()) %>%
  select(model_num, tidied) %>%
  unnest() %>%
  rename("term" = ".rownames", 
         "lower" = "X2.5..", 
         "upper" = "X97.5..")
  
all_params_alt <- left_join(all_params_alt, model_confints)
save(all_params, file = "all_params_alt.rda")

```

In the first plots, we see that our parameter estimates and therefore our conclusions, are unchanged - although, we do not interpret the specific point estimates of each model. 

Plots:
```{r}
top = all_params_alt %>%
  filter(term=="value"|term=="liwc"|term=="audio"|term=="personality"|term=="topic"|term=="linguistic") %>%
  ggplot(aes(model_num, estimate, color = term)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), position="dodge") +
  geom_point(position=position_dodge(width=.9)) +
  scale_colour_manual(values = inferno(6, alpha = 1, begin = .1, end = .9, direction = 1), guide = "legend") +
  scale_fill_manual(values = inferno(6, alpha = 1, begin=.1, end=.9, direction=1), guide=FALSE) +
  labs(x = "", y = "parameter estimate\n") + 
  theme_minimal(base_size = 11) +
  scale_x_continuous(breaks = seq(1, 8, by= 1)) +
  theme(text = element_text(size=17), 
        legend.title = element_text(size = 13),
        legend.text = element_text(size = 11),
        axis.text = element_text(color = "black"),
        axis.line = element_line(colour = "black"),
        legend.position = "top",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank())
top
```

A difference may be noted in the second set of plots. As we are not accounting for the variance related to the task as a random effect, we see a difference when it is estimated as a fixed effect in the model below. 

This can more clearly be observed if the order of the factor levels relating to task are changed: because one level of a factor is used as a reference group against which to compare other factors, changing the reference group allows for different levels of the factor to be estimated. 

Order can be adjusted with the following commented commands:
#levels(test_df$task) <- c("rec_item_cold", "genre_clf", "auto_tag")

What we observe is what appears to be a more accurate estimate for the genre classification task, and that it appears to have a small, positive effect. The other two tasks appear to be poorly estimated, given the wide confidence intervals. 

However, we chose not to include or interpret these models for three reasons: Firstly, we had standardized score within task, secondly we were aware that task was a part of the structure of our dataset, and thirdly our main effects of interest were not affected by whether or not we adjusted the random effects specifications of our model. As such, we elected not to directly interpret the parameter estimate for task in these models. 


```{r}
stan9.plot <- plot_model(stan9.1, ci.lvl=0.95)
stan9.plot <- stan9.plot + theme(text=element_text(size=16)) +
  ggtitle("") +
  labs(y="Parameter Estimates")

#levels(test_df$task) <- c("rec_item_cold", "genre_clf", "auto_tag")
#levels(test_df$task) <- c("auto_tag", "genre_clf", "rec_item_cold")

eff_liwc_task <- plot_model(stan9.1, type = "eff", terms = c("task", "liwc [0, 1]"), ci.lvl=.95) 
eff_value_task <- plot_model(stan9.1, type = "eff", terms = c("task", "value [0, 1]"), ci.lvl=.95)


right_column_plot <- cowplot::plot_grid(eff_liwc_task, eff_value_task, ncol=1, align = "v", axis="l", labels = c('B', 'C'))
plot2 <- cowplot::plot_grid(stan9.plot, right_column_plot, ncol=2, align="h", labels=c('A'))
plot2
```

To determine which terms to include in our models, we had initially used the step function from the lmerTest package. We later learned that this was suboptimal, so we adopted a more robust procedure for variable selection, which is detailed below. Importantly, this was conducted a posteriori, as a validation of our choices. 

To replicate the procedure, first download Jags version 4.x from the source and install:
https://sourceforge.net/projects/mcmc-jags/

## Bayesian Lasso with hierarchical grouping structure

To get a stable selection for the model specification we tried, we conduct a Bayesian linear regression analysis with the Laplace prior following Park and Casella (2008):, such that we can obtain the selective model coefficients. First, we load the relevant libraries (`rjags` and `coda`) and the data.

https://people.eecs.berkeley.edu/~jordan/courses/260-spring09/other-readings/park-casella.pdf

```{r}
library(rjags)
library(coda)

dat <- data.frame(test_df)
dat$task.clf <- as.numeric(dat$task == 'genre_clf')
dat$task.rec <- as.numeric(dat$task == 'rec_item_col')
for (feat in list('audio', 'value', 'topic', 'personality', 'linguistic', 'liwc')) {
    dat[,feat] <- as.numeric(as.factor(dat[,feat])) - 1
}
dat$task = factor(dat$task)
dat$model = factor(dat$model)
dat <- dat %>% group_by(task) %>% mutate(z_score=scale(score))
```

The model structure is as follows:

\[
y_{i} \sim \mathcal{N}(\mu_{i}, \sigma) \\

\mu_{i}=\alpha_0 + \alpha^{t}_{j[i]} + \alpha^{s}_{j[i]k[i]} + x^{0}_{i} \beta_{0} + x^{1}_{i}\beta_{1} + x^{2}_{i}\beta_{2} + \cdots + x^{1}_{i}x^{2}_{i}\beta_{l} + \cdots + \prod_{m\in\mathcal{F}}x^{m}_{i}\beta_{L} \\
\]

Here, we abuse the notation to represent all the individual and interaction terms: $x^0_{i}$ refers the `audio` feature indicator, and following variable-coefficient combinations indicate all the possible interaction terms among the variables of interests. Namely, `value`, `personality`, `topic_modeling`, `LIWC`, `task[genre-clf]`, and finally `task[recsys]` (`task[auto-tagging]` is implicitly coded in other two task-related indicator variables). $\mathcal{F}$ denotes the set of those variables of indication variables and $L=127$ is the total number of individual and interaction terms plus one more term corresponding to the `audio`. Also, another notation abuse is token for indicating the task of $i$th observation (as $j[i]$) and task *and* the system of $i$th observation (as $j[i]k[i]$). Subsequently, the priors of the coefficients are set as follows:

\[
\alpha_0 \sim \mathcal{N}(0, 0.04^{-1}) \\

\alpha^{t}_{j} \sim \mathcal{N}(\mu^{t}_{j}, \sigma^{t}_{j}) \quad j\in\{1, 2, 3\} \\

\alpha^{s}_{jk} \sim \mathcal{N}(0, \sigma^{s}) \quad j\in\{1, 2, 3\}, k\in\{1,...,6\}\\

\beta_{l} \sim Laplace(0, \lambda) \\
\]

Notably, the prior for each effect's $\beta_{l}$ is set as the Laplace distribution such that the model drives the effects to zero. Finally, the hyper-priors are set as mostly flat, non-informative priors, since we do not have any prior knowledge about the data distribution.

\[
\sigma \sim InvGamma(2.5, 25) \\

\mu^{t}_{j} \sim \mathcal{N}(0, 0.04^{-1}) \\

\sigma^{t}_{j} \sim InvGamma(2.5, 25) \\

\sigma^{s} \sim InvGamma(2.5, 25) \\ 

\lambda^{2} \sim Gamma(1, 1)
\]

In R, the model is specified using the JAGS syntax. It could be improved by some tricks, but at the moment we hard-coded all the terms manually.

```{r}
mod1_string = " model {
    for (i in 1:length(y)) {
        y[i] ~ dnorm(mu[i], prec)
        mu[i] = a0 + a_task[task[i]] + a_sys[task[i], sys[i]] + b[1]*audio[i] +
            b[2]*task.clf[i]+b[3]*task.rec[i]+b[4]*value[i]+b[5]*topic[i]+
            b[6]*linguistic[i]+b[7]*liwc[i]+b[8]*personality[i]+
            b[9]*task.clf[i]*task.rec[i]+b[10]*task.clf[i]*value[i]+
            b[11]*task.clf[i]*topic[i]+b[12]*task.clf[i]*linguistic[i]+
            b[13]*task.clf[i]*liwc[i]+b[14]*task.clf[i]*personality[i]+
            b[15]*task.rec[i]*value[i]+b[16]*task.rec[i]*topic[i]+
            b[17]*task.rec[i]*linguistic[i]+b[18]*task.rec[i]*liwc[i]+
            b[19]*task.rec[i]*personality[i]+b[20]*value[i]*topic[i]+
            b[21]*value[i]*linguistic[i]+b[22]*value[i]*liwc[i]+
            b[23]*value[i]*personality[i]+b[24]*topic[i]*linguistic[i]+
            b[25]*topic[i]*liwc[i]+b[26]*topic[i]*personality[i]+
            b[27]*linguistic[i]*liwc[i]+b[28]*linguistic[i]*personality[i]+
            b[29]*liwc[i]*personality[i]+b[30]*task.clf[i]*task.rec[i]*value[i]+
            b[31]*task.clf[i]*task.rec[i]*topic[i]+
            b[32]*task.clf[i]*task.rec[i]*linguistic[i]+b[33]*task.clf[i]*task.rec[i]*liwc[i]+
            b[34]*task.clf[i]*task.rec[i]*personality[i]+
            b[35]*task.clf[i]*value[i]*topic[i]+b[36]*task.clf[i]*value[i]*linguistic[i]+
            b[37]*task.clf[i]*value[i]*liwc[i]+b[38]*task.clf[i]*value[i]*personality[i]+
            b[39]*task.clf[i]*topic[i]*linguistic[i]+b[40]*task.clf[i]*topic[i]*liwc[i]+
            b[41]*task.clf[i]*topic[i]*personality[i]+b[42]*task.clf[i]*linguistic[i]*liwc[i]+
            b[43]*task.clf[i]*linguistic[i]*personality[i]+
            b[44]*task.clf[i]*liwc[i]*personality[i]+b[45]*task.rec[i]*value[i]*topic[i]+
            b[46]*task.rec[i]*value[i]*linguistic[i]+b[47]*task.rec[i]*value[i]*liwc[i]+
            b[48]*task.rec[i]*value[i]*personality[i]+b[49]*task.rec[i]*topic[i]*linguistic[i]+
            b[50]*task.rec[i]*topic[i]*liwc[i]+b[51]*task.rec[i]*topic[i]*personality[i]+
            b[52]*task.rec[i]*linguistic[i]*liwc[i]+b[53]*task.rec[i]*linguistic[i]*personality[i]+
            b[54]*task.rec[i]*liwc[i]*personality[i]+b[55]*value[i]*topic[i]*linguistic[i]+
            b[56]*value[i]*topic[i]*liwc[i]+b[57]*value[i]*topic[i]*personality[i]+
            b[58]*value[i]*linguistic[i]*liwc[i]+b[59]*value[i]*linguistic[i]*personality[i]+
            b[60]*value[i]*liwc[i]*personality[i]+b[61]*topic[i]*linguistic[i]*liwc[i]+
            b[62]*topic[i]*linguistic[i]*personality[i]+b[63]*topic[i]*liwc[i]*personality[i]+
            b[64]*linguistic[i]*liwc[i]*personality[i]+
            b[65]*task.clf[i]*task.rec[i]*value[i]*topic[i]+
            b[66]*task.clf[i]*task.rec[i]*value[i]*linguistic[i]+
            b[67]*task.clf[i]*task.rec[i]*value[i]*liwc[i]+
            b[68]*task.clf[i]*task.rec[i]*value[i]*personality[i]+
            b[69]*task.clf[i]*task.rec[i]*topic[i]*linguistic[i]+
            b[70]*task.clf[i]*task.rec[i]*topic[i]*liwc[i]+
            b[71]*task.clf[i]*task.rec[i]*topic[i]*personality[i]+
            b[72]*task.clf[i]*task.rec[i]*linguistic[i]*liwc[i]+b[73]*task.clf[i]*task.rec[i]*linguistic[i]*personality[i]+
            b[74]*task.clf[i]*task.rec[i]*liwc[i]*personality[i]+b[75]*task.clf[i]*value[i]*topic[i]*linguistic[i]+
            b[76]*task.clf[i]*value[i]*topic[i]*liwc[i]+b[77]*task.clf[i]*value[i]*topic[i]*personality[i]+
            b[78]*task.clf[i]*value[i]*linguistic[i]*liwc[i]+b[79]*task.clf[i]*value[i]*linguistic[i]*personality[i]+
            b[80]*task.clf[i]*value[i]*liwc[i]*personality[i]+b[81]*task.clf[i]*topic[i]*linguistic[i]*liwc[i]+
            b[82]*task.clf[i]*topic[i]*linguistic[i]*personality[i]+
            b[83]*task.clf[i]*topic[i]*liwc[i]*personality[i]+b[84]*task.clf[i]*linguistic[i]*liwc[i]*personality[i]+
            b[85]*task.rec[i]*value[i]*topic[i]*linguistic[i]+b[86]*task.rec[i]*value[i]*topic[i]*liwc[i]+
            b[87]*task.rec[i]*value[i]*topic[i]*personality[i]+b[88]*task.rec[i]*value[i]*linguistic[i]*liwc[i]+
            b[89]*task.rec[i]*value[i]*linguistic[i]*personality[i]+b[90]*task.rec[i]*value[i]*liwc[i]*personality[i]+
            b[91]*task.rec[i]*topic[i]*linguistic[i]*liwc[i]+b[92]*task.rec[i]*topic[i]*linguistic[i]*personality[i]+
            b[93]*task.rec[i]*topic[i]*liwc[i]*personality[i]+b[94]*task.rec[i]*linguistic[i]*liwc[i]*personality[i]+
            b[95]*value[i]*topic[i]*linguistic[i]*liwc[i]+b[96]*value[i]*topic[i]*linguistic[i]*personality[i]+
            b[97]*value[i]*topic[i]*liwc[i]*personality[i]+b[98]*value[i]*linguistic[i]*liwc[i]*personality[i]+
            b[99]*topic[i]*linguistic[i]*liwc[i]*personality[i]+
            b[100]*task.clf[i]*task.rec[i]*value[i]*topic[i]*linguistic[i]+
            b[101]*task.clf[i]*task.rec[i]*value[i]*topic[i]*liwc[i]+
            b[102]*task.clf[i]*task.rec[i]*value[i]*topic[i]*personality[i]+
            b[103]*task.clf[i]*task.rec[i]*value[i]*linguistic[i]*liwc[i]+
            b[104]*task.clf[i]*task.rec[i]*value[i]*linguistic[i]*personality[i]+
            b[105]*task.clf[i]*task.rec[i]*value[i]*liwc[i]*personality[i]+
            b[106]*task.clf[i]*task.rec[i]*topic[i]*linguistic[i]*liwc[i]+
            b[107]*task.clf[i]*task.rec[i]*topic[i]*linguistic[i]*personality[i]+
            b[108]*task.clf[i]*task.rec[i]*topic[i]*liwc[i]*personality[i]+
            b[109]*task.clf[i]*task.rec[i]*linguistic[i]*liwc[i]*personality[i]+
            b[110]*task.clf[i]*value[i]*topic[i]*linguistic[i]*liwc[i]+
            b[111]*task.clf[i]*value[i]*topic[i]*linguistic[i]*personality[i]+
            b[112]*task.clf[i]*value[i]*topic[i]*liwc[i]*personality[i]+
            b[113]*task.clf[i]*value[i]*linguistic[i]*liwc[i]*personality[i]+
            b[114]*task.clf[i]*topic[i]*linguistic[i]*liwc[i]*personality[i]+
            b[115]*task.rec[i]*value[i]*topic[i]*linguistic[i]*liwc[i]+
            b[116]*task.rec[i]*value[i]*topic[i]*linguistic[i]*personality[i]+
            b[117]*task.rec[i]*value[i]*topic[i]*liwc[i]*personality[i]+
            b[118]*task.rec[i]*value[i]*linguistic[i]*liwc[i]*personality[i]+
            b[119]*task.rec[i]*topic[i]*linguistic[i]*liwc[i]*personality[i]+
            b[120]*value[i]*topic[i]*linguistic[i]*liwc[i]*personality[i]+
            b[121]*task.clf[i]*task.rec[i]*value[i]*topic[i]*linguistic[i]*liwc[i]+
            b[122]*task.clf[i]*task.rec[i]*value[i]*topic[i]*linguistic[i]*personality[i]+
            b[123]*task.clf[i]*task.rec[i]*value[i]*topic[i]*liwc[i]*personality[i]+
            b[124]*task.clf[i]*task.rec[i]*value[i]*linguistic[i]*liwc[i]*personality[i]+
            b[125]*task.clf[i]*task.rec[i]*topic[i]*linguistic[i]*liwc[i]*personality[i]+
            b[126]*task.clf[i]*value[i]*topic[i]*linguistic[i]*liwc[i]*personality[i]+
            b[127]*task.rec[i]*value[i]*topic[i]*linguistic[i]*liwc[i]*personality[i]
    }

    for (j in 1:3) {
        a_task[j] ~ dnorm(c_mu[j], c_sig[j])
        for (k in 1:6) {
            a_sys[j, k] ~ dnorm(0, sig_sys)
        }
    }
    a0 ~ dnorm(0.0, 1./25.)
    
    for (l in 1:127) {
        b[l] ~ ddexp(0.0, sqrt(lam_sqr)) # has variance 1.0
    }
    
    for (j in 1:3) {
        c_mu[j] ~ dnorm(0., 1./25.)
        c_sig[j] ~ dgamma(5/2., 5.*10./2.)
    }
    sig_sys ~ dgamma(5/2., 5.*10./2.)
    
    lam_sqr ~ dgamma(1, 1)
    prec ~ dgamma(5/2.0, 5*10.0/2.0)
    sig = sqrt( 1.0 / prec )
} "
```

Although we set a seed in rjags, the seed in jags itself has not been set. As such, results will vary. 

```{r}
set.seed(116)
data_jags = list(y=as.numeric(dat$z_score),
                 audio=as.numeric(dat$audio),
                 topic=as.numeric(dat$topic),
                 personality=as.numeric(dat$personality),
                 liwc=as.numeric(dat$liwc),
                 linguistic=as.numeric(dat$linguistic),
                 value=as.numeric(dat$value),
                 sys=as.numeric(dat$model),
                 task=as.numeric(dat$task),
                 task.clf=as.numeric(dat$task.clf),
                 task.rec=as.numeric(dat$task.rec))
# table(data_jags$is_oil, data_jags$region)

params = c("a0", "a_sys", "a_task", "b", "sig", "lam_sqr", "c_mu", "c_sig", "sig_sys")

mod = jags.model(textConnection(mod1_string), data=data_jags, n.chains=3)
update(mod, 1e3) # burn-in
```

The next process will take ***some hours*** to run. To look at the results of our previous run, simply load the object here:

```{r}
base::load(here("mod_sim.rda"))
```

Otherwise run the complete process here:

```{r}
mod_sim = coda.samples(model=mod,
                       variable.names=params,
                       n.iter=1e5)

save(mod_sim, file = "mod_sim.rda")
mod_csim = as.mcmc(do.call(rbind, mod_sim), thin=1) # combine multiple chains
```

Figures and visualizations:

```{r}
## convergence diagnostics
graphics.off()
 par("mar")
 par(mar=c(1,1,1,1))
plot(mod_sim)
```

We run a few diagnostics for the model convergence. It seems we need to a longer run (at least 5~6 times) according to Gelman's diganostics. The auto-correlation diagnostics also suggests it needs a longer run. We suppose it's likely because the nested structure introduced in the experiment does not have a strong grouping effect. But to ensure this we might need a further study.

```{r}
gelman.diag(mod_sim)
autocorr.diag(mod_sim)
autocorr.plot(mod_sim)
effectiveSize(mod_sim)
```

Now we can print out the credible interval for all the effects, to see how they're robust.

```{r}
# here we compute the credible interval at 95% credibility
HPDinterval(mod_csim, prob=0.9)
```

With the rather explorative regression analysis, we found the posterior distribution of a subset of variables do not overlap to $0$ at 90% credibility. Namely:

- `audio`
- `value` (it's almost also sure at the 95% level, although it's also not always dodging it, but also our model has not fully converged yet.)
- `liwc`
- `liwc:task[genre-clf]`



